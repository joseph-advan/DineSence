# customer_analysis_mvp/ui/video_view.py

import streamlit as st
import os
import time
import cv2
from collections import Counter

# å°å…¥æˆ‘å€‘çš„æœå‹™æ¨¡çµ„
from services import vision_analysis as va
from services import llm_handler as llm

def display(client, menu_items, llm_preferences):
    """
    é¡¯ç¤ºã€Œå½±ç‰‡é›¢ç·šåˆ†æã€åˆ†é çš„æ‰€æœ‰ UI å…ƒç´ èˆ‡åˆ†æé‚è¼¯ã€‚
    
    Args:
        client (OpenAI): åˆå§‹åŒ–å¾Œçš„ OpenAI å®¢æˆ¶ç«¯ç‰©ä»¶ã€‚
        menu_items (list): å¾å´é‚Šæ¬„ç²å–çš„èœå–®é …ç›®åˆ—è¡¨ã€‚
        llm_preferences (dict): åŒ…å« store_type, tone, tips_style çš„å­—å…¸ã€‚
    """
    st.subheader("ğŸï¸ ä¸Šå‚³å½±ç‰‡é€²è¡Œé›¢ç·šåˆ†æèˆ‡æ‘˜è¦")
    up = st.file_uploader("æ”¯æ´ .mp4 / .avi æ ¼å¼", type=["mp4", "avi"])

    col1, col2, col3 = st.columns(3)
    with col1:
        sample_sec = st.number_input("æŠ½æ¨£é–“éš”ï¼ˆç§’ï¼‰", min_value=1, max_value=30, value=5, step=1, help="æ¯éš”å¹¾ç§’åˆ†æä¸€å¼µç•«é¢")
    with col2:
        do_plate_v = st.checkbox("åˆ†æé¤ç›¤æ®˜ç•™", value=True)
    with col3:
        do_emote_v = st.checkbox("åˆ†æè¡¨æƒ…", value=True)

    do_food_v = st.checkbox("åˆ†æé£Ÿç‰©/é£²å“ï¼ˆYOLOâ†’èœå–®åˆ†é¡ï¼‰", value=True)

    if up is not None:
        # ç‚ºäº†è™•ç†ä¸Šå‚³çš„æª”æ¡ˆï¼Œå…ˆå°‡å…¶å¯«å…¥æš«å­˜æª”
        tmp_path = os.path.join(".", f"tmp_{up.name}")
        with open(tmp_path, "wb") as f:
            f.write(up.getbuffer())

        st.video(tmp_path)
        
        if st.button("ğŸš€ é–‹å§‹åˆ†æå½±ç‰‡", type="primary", use_container_width=True, disabled=not client):
            progress_bar = st.progress(0, text="æº–å‚™é–‹å§‹åˆ†æ...")
            
            # åˆå§‹åŒ–åµæ¸¬å™¨èˆ‡è¨ˆæ•¸å™¨
            pose_detector = va.get_pose_detector()
            face_detector = va.get_face_detector()
            nod_detector = va.NodDetector()
            
            leftover_counter = Counter()
            emotion_counter = Counter()
            food_counter = Counter()
            nod_total = 0
            timeline = []

            cap = cv2.VideoCapture(tmp_path)
            fps = cap.get(cv2.CAP_PROP_FPS) or 30
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            step = max(1, int(fps * sample_sec))

            try:
                for fr in range(0, total_frames, step):
                    cap.set(cv2.CAP_PROP_POS_FRAMES, fr)
                    ok, frame = cap.read()
                    if not ok:
                        break

                    progress_text = f"åˆ†æé€²åº¦ï¼š{fr}/{total_frames} frames..."
                    progress_bar.progress(fr / total_frames, text=progress_text)

                    timestamp_s = int(fr / fps)
                    mm_ss = f"{timestamp_s//60:02d}:{timestamp_s%60:02d}"

                    # --- åŸ·è¡Œå„é …åˆ†æ ---
                    plate_label, food_label, emo_label, nod_flag = "-", "-", "-", 0
                    
                    if do_plate_v:
                        label, _, _ = va.estimate_plate_leftover(frame)
                        if label in ["å‰©é¤˜50%ä»¥ä¸Š", "ç„¡å‰©é¤˜"]:
                            leftover_counter[label] += 1
                            plate_label = label

                    if do_food_v:
                        regions = va.detect_food_regions_yolo(frame, conf=0.3, min_area_ratio=0.01)[:1]
                        if regions:
                            if menu_items and client:
                                x1, y1, x2, y2 = regions[0]["xyxy"]
                                crop = frame[y1:y2, x1:x2].copy()
                                res_food = llm.gpt_food_from_menu(crop, menu_items, client)
                                food_label = res_food["label"]
                            else:
                                food_label = regions[0]["label"]
                            food_counter[food_label] += 1

                    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    res = pose_detector.process(rgb)
                    if res.pose_landmarks:
                        lm = res.pose_landmarks.landmark
                        nose_y = lm[0].y
                        ref_y = (lm[11].y + lm[12].y) / 2 if len(lm) > 12 else 0.5
                        if nod_detector.update_and_check(nose_y, ref_y):
                            nod_total += 1
                            nod_flag = 1
                    
                    if do_emote_v:
                        face_crop = va.crop_face_with_mediapipe(frame, face_detector)
                        emo = llm.gpt_image_classify_3cls(face_crop, client)
                        if emo in ["å–œæ­¡", "ä¸­æ€§", "è¨å­"]:
                            emotion_counter[emo] += 1
                            emo_label = emo
                    
                    timeline.append({
                        "t": mm_ss, "leftover": plate_label, "food": food_label, 
                        "nod": "âœ”" if nod_flag else " ", "emotion": emo_label
                    })

                progress_bar.progress(1.0, text="åˆ†æå®Œæˆï¼æ­£åœ¨ç”¢ç”Ÿæ‘˜è¦...")

                stats = {
                    "leftover": dict(leftover_counter), "food": dict(food_counter),
                    "nod": nod_total, "emotion": dict(emotion_counter),
                    "timeline": timeline
                }
                
                with st.expander("æŸ¥çœ‹åŸå§‹çµ±è¨ˆæ•¸æ“š (JSON)", expanded=False):
                    st.json(stats)
                
                summary = llm.summarize_session(stats, **llm_preferences, client=client)
                st.success("ğŸ¯ å½±ç‰‡åˆ†ææ‘˜è¦")
                st.markdown(summary)

            finally:
                cap.release()
                os.remove(tmp_path) # åˆªé™¤æš«å­˜æª”
                progress_bar.empty() # æ¸…ç†é€²åº¦æ¢